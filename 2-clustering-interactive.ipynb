{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import math\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.constants import RANDOM_SEED\n",
    "from source.constants import NUM_CLASS_IMAGES, NUM_CLASS_PROTOTYPES, AVG_NUM_DUPLICATES_PER_CLASS_PROTOTYPE\n",
    "from source.constants import DATA_DIR, FEATURE_VECTORS_SAVE_DIR, ANNOTATIONS_SAVE_DIR\n",
    "from source.constants import ALL_CANCER_TYPES, ALL_EXTRACTOR_MODELS, ALL_IMG_NORMS, ALL_DISTANCE_METRICS, ALL_DIMENSIONALITY_REDUCTION_METHODS, ALL_CLUSTERING_ALGORITHMS\n",
    "\n",
    "print(\"RANDOM_SEED:\", RANDOM_SEED)\n",
    "print(\"NUM_CLASS_IMAGES:\", NUM_CLASS_IMAGES)\n",
    "print(\"NUM_CLASS_PROTOTYPES:\", NUM_CLASS_PROTOTYPES)\n",
    "print(\"AVG_NUM_DUPLICATES_PER_CLASS_PROTOTYPE:\", AVG_NUM_DUPLICATES_PER_CLASS_PROTOTYPE)\n",
    "print()\n",
    "\n",
    "print(f\"DATA_DIR: {DATA_DIR}\")\n",
    "print(f\"FEATURE_VECTORS_SAVE_DIR: {FEATURE_VECTORS_SAVE_DIR}\")\n",
    "print(f\"ANNOTATIONS_SAVE_DIR: {ANNOTATIONS_SAVE_DIR}\")\n",
    "print()\n",
    "\n",
    "print(\"ALL_CANCER_TYPES:\", ALL_CANCER_TYPES)\n",
    "print(\"ALL_EXTRACTOR_MODELS:\", ALL_EXTRACTOR_MODELS)\n",
    "print(\"ALL_DISTANCE_METRICS:\", ALL_DISTANCE_METRICS)\n",
    "print(\"ALL_DIMENSIONALITY_REDUCTION_METHODS:\", ALL_DIMENSIONALITY_REDUCTION_METHODS)\n",
    "print(\"ALL_CLUSTERING_ALGORITHMS:\", ALL_CLUSTERING_ALGORITHMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.eval_utils import reduce_feature_dimensionality, get_clustering_labels, get_clustering_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.interactive_clustering_utils import (\n",
    "    visualise_kmeans_cluster,\n",
    "    display_image_pairs,            # manual accepting and rejecting of pairs: anchor, candidate\n",
    "    visualize_cluster_results_processed_file_path,\n",
    "    kmeans_and_review,              # cluster and purify rejected images\n",
    "    merge_clusters_interactively,   # merge pure accepted and pure rejected clusters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set the cancer type and extractor name\n",
    "CANCER_TYPE = 'colon_n'\n",
    "EXTRACTOR_NAME = 'UNI'\n",
    "IMG_NORM = 'resize_only'\n",
    "DISTANCE_METRIC = 'euclidean'\n",
    "DIMENSIONALITY_REDUCTION_METHOD = 'UMAP-8'\n",
    "CLUSTERING_ALGORITHM = 'kmeans'\n",
    "\n",
    "assert CANCER_TYPE in ALL_CANCER_TYPES\n",
    "assert EXTRACTOR_NAME in ALL_EXTRACTOR_MODELS\n",
    "assert IMG_NORM in ALL_IMG_NORMS\n",
    "assert CLUSTERING_ALGORITHM in ALL_CLUSTERING_ALGORITHMS\n",
    "assert DIMENSIONALITY_REDUCTION_METHOD in ALL_DIMENSIONALITY_REDUCTION_METHODS\n",
    "assert DISTANCE_METRIC in ALL_DISTANCE_METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_save_dir = f'{FEATURE_VECTORS_SAVE_DIR}/{CANCER_TYPE}/{EXTRACTOR_NAME}/{IMG_NORM}'\n",
    "print(\"features_save_dir:\", features_save_dir)\n",
    "\n",
    "# expected to already be there\n",
    "features_npy_path = f'{features_save_dir}/features.npy'\n",
    "ids_2_imgpaths_json_path = f'{features_save_dir}/ids_2_img_paths.json'\n",
    "assert os.path.isfile(features_npy_path)\n",
    "assert os.path.isfile(ids_2_imgpaths_json_path)\n",
    "\n",
    "# to save objects created in this notebook\n",
    "results_dir = features_save_dir.replace(FEATURE_VECTORS_SAVE_DIR, ANNOTATIONS_SAVE_DIR)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "clustering_model_path = f'{results_dir}/distance_metric={DISTANCE_METRIC}#dimensionality_reduction={DIMENSIONALITY_REDUCTION_METHOD}#clustering={CLUSTERING_ALGORITHM}.pkl'\n",
    "print(\"clustering_model_path:\", clustering_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load(features_npy_path)\n",
    "print(features.shape)\n",
    "\n",
    "# Load image paths\n",
    "with open(ids_2_imgpaths_json_path, 'r') as f:\n",
    "    ids_2_imgpaths = json.load(f)\n",
    "print(ids_2_imgpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DISTANCE_METRIC == 'cosine':\n",
    "    # after normalisation, euclidean distance is equivalent to cosine distance\n",
    "    # KMeans does not support cosine distance, so we can't just pass distance_metric to KMeans as a parameter\n",
    "    features = features / \\\n",
    "        np.linalg.norm(features, axis=1,  keepdims=True)\n",
    "\n",
    "features = reduce_feature_dimensionality(features=features, method=DIMENSIONALITY_REDUCTION_METHOD)\n",
    "print(features.shape, features.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "labels, clustering_model = get_clustering_labels(\n",
    "    features=features,\n",
    "    n_clusters=NUM_CLASS_PROTOTYPES,\n",
    "    method=CLUSTERING_ALGORITHM,\n",
    "    random_state=RANDOM_SEED,\n",
    "    return_model=True\n",
    ")\n",
    "cluster_centers = get_clustering_centroids(features, labels)\n",
    "print(labels.shape, cluster_centers.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(clustering_model_path):\n",
    "    print(f\"No model at {clustering_model_path}. \\n Saving the current model.\")\n",
    "    with open(clustering_model_path, 'wb') as f:\n",
    "        pickle.dump(clustering_model, f)\n",
    "        labels = clustering_model.labels_\n",
    "else:\n",
    "    print(f\"KMeans model at {clustering_model_path} exists. \\n Loading and checking if it's the same as the current model.\")\n",
    "    with open(clustering_model_path, 'rb') as f:\n",
    "        loaded_kmeans = pickle.load(f)\n",
    "    if (\n",
    "        (clustering_model.labels_ != loaded_kmeans.labels_).all()\n",
    "        or not np.allclose(clustering_model.cluster_centers_,\n",
    "                           loaded_kmeans.cluster_centers_,\n",
    "                           atol=1e-6)\n",
    "    ):\n",
    "        print(\"Loaded KMeans model is not the same as the current model.\")\n",
    "        user_input = input(\"Do you want to use the loaded model (l) or the new model (n)?\")\n",
    "\n",
    "        if user_input == 'n':\n",
    "            labels = clustering_model.labels_\n",
    "        elif user_input == 'l':\n",
    "            labels = loaded_kmeans.labels_\n",
    "        else:\n",
    "            raise NotImplementedError(\"Choose between 'l' and 'n'.\")\n",
    "    else:\n",
    "        print(\"Loaded KMeans model is the same as the current model.\")\n",
    "        labels = loaded_kmeans.labels_\n",
    "        \n",
    "print(labels)\n",
    "\n",
    "# kmeans has the centroids attribute, but agglomerative clustering does not\n",
    "centroids = get_clustering_centroids(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map cluster labels to image paths and features\n",
    "labels_2_imgpaths = {}\n",
    "labels_2_features = {}\n",
    "for idx, label in enumerate(labels):\n",
    "    img_path = ids_2_imgpaths[str(idx)]\n",
    "    if label not in labels_2_imgpaths:\n",
    "        labels_2_imgpaths[label] = []\n",
    "        labels_2_features[label] = []\n",
    "    labels_2_imgpaths[label].append(img_path)\n",
    "    labels_2_features[label].append(features[idx])\n",
    "\n",
    "# Find the image closest to the centroid for each cluster\n",
    "centroid_imgpaths = {}\n",
    "for label, centroid in enumerate(centroids):\n",
    "    distances = euclidean_distances([centroid], labels_2_features[label])\n",
    "    closest_idx = np.argmin(distances)\n",
    "    centroid_imgpaths[label] = labels_2_imgpaths[label][closest_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_kmeans_cluster(\n",
    "    cluster_index=2,\n",
    "    labels_2_imgpaths=labels_2_imgpaths,\n",
    "    labels_2_features=labels_2_features,\n",
    "    centroids=centroids,\n",
    "    num_examples=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for results and session state\n",
    "results_csv_file_path = f'{results_dir}/results.csv'\n",
    "session_state_file_path = f'{results_dir}/session_state.json'\n",
    "results_processed_file_path = f'{results_dir}/results_processed.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the session from the last state\n",
    "for i in range(5):\n",
    "    print(f\"Cluster {i}\")\n",
    "    interrupt_status = display_image_pairs(\n",
    "        labels_2_imgpaths=labels_2_imgpaths,\n",
    "        labels_2_features=labels_2_features,\n",
    "        features=features,\n",
    "        centroid_imgpaths=centroid_imgpaths,\n",
    "        results_csv_file_path=results_csv_file_path,\n",
    "        results_processed_file_path=results_processed_file_path,\n",
    "        session_state_file_path=session_state_file_path,\n",
    "    )\n",
    "    if interrupt_status == 'q':\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Skipped and Non-belonging images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed images\n",
    "with open(results_processed_file_path, 'r') as f:\n",
    "    results_processed = json.load(f)\n",
    "\n",
    "processed_images_paths = []\n",
    "non_belonging_images_paths = []\n",
    "for anchor_img_path, details in results_processed.items():\n",
    "    processed_images_paths.append(anchor_img_path)\n",
    "    processed_images_paths.extend(details['belonging_image_paths'])\n",
    "    processed_images_paths.extend(details['non_belonging_image_paths'])\n",
    "    non_belonging_images_paths.extend(details['non_belonging_image_paths'])\n",
    "\n",
    "assert len(set(processed_images_paths)) == len(processed_images_paths)\n",
    "assert len(set(non_belonging_images_paths)) == len(non_belonging_images_paths)\n",
    "\n",
    "processed_images_paths = sorted(processed_images_paths)\n",
    "non_belonging_images_paths = sorted(non_belonging_images_paths)\n",
    "\n",
    "print(f\"Total processed images: {len(processed_images_paths)}\")\n",
    "print(f\"Total non-belonging images: {len(non_belonging_images_paths)}\")\n",
    "\n",
    "# Get all images in clusters\n",
    "all_images_paths = []\n",
    "for img_list in labels_2_imgpaths.values():\n",
    "    all_images_paths.extend(img_list)\n",
    "assert len(set(all_images_paths)) == len(all_images_paths)\n",
    "all_images_paths = sorted(all_images_paths)\n",
    "\n",
    "# Determine skipped images\n",
    "skipped_images_paths = set(all_images_paths) - set(processed_images_paths)\n",
    "print(f\"Skipped Images: {len(set(skipped_images_paths))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "visualize_cluster_results_processed_file_path(\n",
    "    cluster_id=0,\n",
    "    results_processed_file_path=results_processed_file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage - \n",
    "visualize_cluster_results_processed_file_path(cluster_id=60, results_processed_file_path=results_processed_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Non-Belonging Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_rejected_clusters_json_path = f'{results_dir}/pure_rejected_clusters.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths_2_int_ids = {v: int(k) for k, v in ids_2_imgpaths.items()}\n",
    "\n",
    "# Extract features for the non-belonging images\n",
    "non_belonging_features = [features[img_paths_2_int_ids[img_path]]\n",
    "                          for img_path in non_belonging_images_paths]\n",
    "\n",
    "# Convert to numpy array\n",
    "non_belonging_features = np.array(non_belonging_features)\n",
    "print(non_belonging_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_clusters = kmeans_and_review(\n",
    "    non_belonging_features=non_belonging_features,\n",
    "    n_clusters=math.ceil(2 * len(non_belonging_images_paths) / AVG_NUM_DUPLICATES_PER_CLASS_PROTOTYPE), # make twice as many clusters as expected to increase the chance of finding pure clusters\n",
    "    non_belonging_images_paths=non_belonging_images_paths,\n",
    "    pure_rejected_clusters_json_path=pure_rejected_clusters_json_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Rejected Purified Clusters and Originally-selected Clusters until there are <= 250 clusters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pure_rejected_clusters_json_path, 'r') as f:\n",
    "    pure_rejected_clusters = json.load(f)\n",
    "\n",
    "assert sum(len(v) for v in pure_rejected_clusters.values()\n",
    "           ) == len(non_belonging_images_paths)\n",
    "print(f\"Total pure clusters: {len(pure_rejected_clusters)}\")\n",
    "print(\n",
    "    f\"Total rejected images: {sum(len(v) for v in pure_rejected_clusters.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "# Load features\n",
    "features = np.load(features_npy_path)\n",
    "\n",
    "# load ids_2_imgpaths\n",
    "with open(ids_2_imgpaths_json_path, 'r') as f:\n",
    "    ids_2_imgpaths = json.load(f)\n",
    "img_paths_2_int_ids = {v: int(k) for k, v in ids_2_imgpaths.items()}\n",
    "\n",
    "# Load the merged clusters\n",
    "with open(pure_rejected_clusters_json_path, 'r') as f:\n",
    "    pure_rejected_clusters = json.load(f)\n",
    "\n",
    "# load the results_processed\n",
    "with open(f'{results_dir}/results_processed.json', 'r') as f:\n",
    "    results_processed = json.load(f)\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "all_clusters = {}\n",
    "for key, value in results_processed.items():\n",
    "    cluster_id = int(value['cluster_index'])\n",
    "    # key is the anchor image path\n",
    "    images = [key] + value['belonging_image_paths']\n",
    "    all_clusters[cluster_id] = images\n",
    "\n",
    "for cluster_id, images in pure_rejected_clusters.items():\n",
    "    all_clusters[NUM_CLASS_PROTOTYPES + int(cluster_id)] = images\n",
    "\n",
    "# just to make sure we are not modifying the original clusters\n",
    "all_clusters = deepcopy(all_clusters)\n",
    "\n",
    "print(\"Total clusters:\", len(all_clusters))\n",
    "\n",
    "total = 0\n",
    "for key, value in all_clusters.items():\n",
    "    total += len(value)\n",
    "    # print(f\"Cluster {key}: {len(value)} images\")\n",
    "print(f\"Total images: {total}\")\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "# Execute the interactive merging\n",
    "merge_results = merge_clusters_interactively(\n",
    "    clusters=all_clusters,\n",
    "    features=features,\n",
    "    img_paths_2_int_ids=img_paths_2_int_ids,\n",
    "    max_num_clusters=250,\n",
    "    linkage='single',\n",
    "    patience=10,\n",
    ")\n",
    "\n",
    "final_clusters = merge_results['clusters']\n",
    "print(\"Final clusters:\", final_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be interrupted and restarted\n",
    "merge_results = merge_clusters_interactively(\n",
    "    clusters=final_clusters,\n",
    "    features=features,\n",
    "    img_paths_2_int_ids=img_paths_2_int_ids,\n",
    "    max_num_clusters=250,\n",
    "    linkage='single',\n",
    "    patience=10,\n",
    ")\n",
    "\n",
    "final_clusters = merge_results['clusters']\n",
    "print(\"Final clusters:\", final_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last check and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_clusters.keys())\n",
    "total = 0\n",
    "uniqie_images = set()\n",
    "images_per_cluster_list = []\n",
    "for key, value in final_clusters.items():\n",
    "    total += len(value)\n",
    "    # print(f\"Cluster {key}: {len(value)} images\")\n",
    "    uniqie_images.update(value)\n",
    "    images_per_cluster_list.append(len(value))\n",
    "print(total)\n",
    "assert total == 5000\n",
    "assert len(uniqie_images) == 5000\n",
    "\n",
    "print(f\"Total images: {total}\")\n",
    "print(f\"Unique images: {len(uniqie_images)}\")\n",
    "\n",
    "plt.bar(range(len(images_per_cluster_list)), images_per_cluster_list)\n",
    "plt.xlabel('Cluster Index')\n",
    "plt.ylabel('Number of Images')\n",
    "\n",
    "\n",
    "# Save the final clusters in a csv file, make sure to save the labels as integers from 0 to n_clusters-1 for consistency\n",
    "final_clusters_csv_path = f'{results_dir}/final_clusters.csv'\n",
    "final_clusters_contiguous_indices = {}\n",
    "with open(final_clusters_csv_path, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['cluster_label', 'img_path'])\n",
    "\n",
    "    for i, key in enumerate(final_clusters.keys()):\n",
    "        \n",
    "        # access the list of files at key\n",
    "        sorted_img_paths = sorted(final_clusters[key])\n",
    "        # save the sorted_img_paths in the final_clusters_contiguous_indices with the new key\n",
    "        final_clusters_contiguous_indices[i] = sorted_img_paths\n",
    "\n",
    "        for img_path in sorted_img_paths:\n",
    "            csvwriter.writerow([i, img_path])\n",
    "\n",
    "# Save the final clusters in a json file\n",
    "final_clusters_json_path = f'{results_dir}/final_clusters.json'\n",
    "with open(final_clusters_json_path, 'w') as f:\n",
    "    json.dump(final_clusters_contiguous_indices, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc25k-cleaning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
